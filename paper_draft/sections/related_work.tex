\section{Related Work}
\label{sec:related_work}

\paragraph{Multi-turn performance degradation.}
A growing body of work demonstrates that LLM performance degrades in multi-turn settings. \citet{laban2025lic} find a 39\% average performance drop across 15 LLMs in multi-turn conversations, decomposing it into minor aptitude loss ($-$15\%) and major unreliability increase (+112\%). \citet{kwan2024mteval} and \citet{he2024multiif} provide benchmarks confirming this trend across instruction following and multilingual settings. \citet{khalid2025ergo} show that entropy spikes signal misalignment points in multi-turn interactions and propose adaptive prompt consolidation to mitigate degradation. Unlike these works, which document \emph{that} degradation occurs, we test \emph{whether} it represents regression toward the base model prior.

\paragraph{Context drift and equilibria.}
\citet{dongre2025drift} formalize context drift as turn-wise divergence from a goal-consistent reference distribution, showing that drift stabilizes at bounded equilibria rather than growing unboundedly. Simple reminder interventions reduce drift by 7--67\%. Their framework distinguishes context drift (information loss) from alignment drift (value deviation), but does not test whether models converge toward base model behavior. We build on this distinction by designing interventions that differentially target alignment versus context effects.

\paragraph{Multi-turn safety erosion.}
Several works demonstrate that safety alignment weakens over successive turns. Crescendo \citep{russinovich2024crescendo} achieves 97--100\% jailbreak success through gradual escalation. \citet{zhou2025tempest} and \citet{weng2025fitd} show similar results with tree-search and foot-in-the-door strategies. \citet{singhania2025mmart} find 71--195\% increased vulnerability after five turns, with greater erosion in non-English languages. These results demonstrate that even heavily reinforced safety training fails to persist, but they do not measure whether the resulting behavior resembles the base model or represents a qualitatively different failure mode.

\paragraph{Sycophancy in multi-turn settings.}
Sycophancy---the tendency to agree with users regardless of correctness---has emerged as a key multi-turn vulnerability. \citet{liu2025truthdecay} show progressive compromise of factual accuracy in extended dialogues. \citet{hong2025syconbench} find that alignment tuning \emph{amplifies} sycophantic behavior, while model scaling reduces it. The FlipFlop benchmark shows that models flip their answers 46\% of the time when challenged with ``Are you sure?'' \citep{flipflop2023}. Critically, \citet{pan2025userassistant} demonstrate that user-deference bias is \emph{created by} alignment training (DPO/RLHF) and absent in base models, with bias scores of 0.7--0.97 for instruction-tuned models versus $\sim$0.0 for base models. This suggests sycophancy is an alignment artifact rather than base model behavior. Our work directly tests this hypothesis in multi-turn settings through diagnostic interventions.

\paragraph{RLHF and multi-turn optimization.}
\citet{wang2023mint} provide perhaps the most direct evidence that alignment hurts multi-turn capability, finding that RLHF and supervised instruction fine-tuning generally degrade multi-turn performance. \citet{gao2024refuel} address this by proposing multi-turn-aware RLHF that accounts for future turns, showing that an 8B model can outperform 70B models at turns 3+. These findings suggest that standard alignment training is fundamentally single-turn-biased, motivating our investigation of whether its effects attenuate over conversation depth.

\paragraph{Positioning our work.}
Existing work extensively documents multi-turn degradation across performance, safety, and sycophancy dimensions. However, no prior study directly tests whether this degradation represents convergence toward the base model's behavioral distribution. The closest work---\citet{dongre2025drift} on context drift and \citet{pan2025userassistant} on user-assistant bias---measures divergence from reference policies and identifies alignment-induced biases, but neither tracks whether aligned models \emph{revert} toward base behavior over turns. We fill this gap by measuring alignment-specific behavioral markers across conversation depths and using diagnostic interventions to distinguish alignment attenuation from context information loss.
