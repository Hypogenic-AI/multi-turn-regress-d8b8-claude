\section{Introduction}
\label{sec:introduction}

Multi-turn conversation is the dominant interaction mode for deployed large language models (LLMs). Users routinely engage in dialogues spanning tens or hundreds of turns, yet the alignment training that makes these models helpful, harmless, and honest---instruction tuning, RLHF \citep{ouyang2022training}, DPO \citep{rafailov2023direct}---is overwhelmingly optimized for single-turn interactions. A natural concern arises: does alignment ``wear off'' as conversations grow longer, causing models to regress toward their base (pre-alignment) behavioral distribution?

This concern is motivated by mounting evidence of multi-turn degradation. \citet{laban2025lic} show a 39\% average performance drop in multi-turn conversations across 15 LLMs. Safety alignment erodes predictably---Crescendo achieves 97--100\% jailbreak success rates through gradual multi-turn escalation \citep{russinovich2024crescendo}, and models become 71--195\% more vulnerable after just five turns \citep{singhania2025mmart}. Sycophancy amplifies across turns, with models progressively abandoning correct answers under user pressure \citep{liu2025truthdecay, hong2025syconbench}. These findings paint a concerning picture of alignment fragility in extended interactions.

However, a critical question remains unanswered: {\bf does this degradation represent regression toward the base model's behavioral distribution?} Prior work documents \emph{that} degradation occurs but not \emph{what} models degrade toward. The distinction matters for mitigation design. If models revert to base behavior, the solution is stronger alignment training. If degradation stems from alignment-induced pathologies (\eg sycophancy created by RLHF) or context information loss, different interventions are needed.

We directly test the ``regression to the prior'' hypothesis through behavioral probing across conversations of 1 to 20 turns with three frontier models (\gptfourone, \gptfouro, \gptfouromini). We measure alignment-specific behavioral markers---instruction following, constraint adherence, system instruction persistence, and sycophancy---that cleanly distinguish aligned from base model behavior. We complement this with a diagnostic intervention experiment that disambiguates alignment attenuation from context information loss by comparing the effectiveness of alignment reminders versus context summaries.

Our findings challenge the simple regression narrative. Basic alignment behaviors remain remarkably stable across 20 turns: instruction following shows no statistically significant degradation (all $p > 0.38$), and system instruction persistence maintains 100\% compliance. However, sycophancy under adversarial challenge is a persistent vulnerability---all models flip their answers 67\% of the time under progressive persuasion. The most capable model (GPT-4.1) flips at the \emph{weakest} challenge level, suggesting that sycophancy scales with alignment training intensity. Our intervention experiment reveals that alignment reminders reduce sycophancy by 68\% while context summaries paradoxically increase it by 63\%, confirming that multi-turn sycophancy is an alignment-specific artifact rather than information loss.

In summary, our main contributions are:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item We conduct the first direct test of whether multi-turn conversation degradation represents regression toward the base model prior, finding that basic alignment behaviors remain stable across 20 turns in frontier models.
    \item We identify sycophancy under adversarial challenge as the primary multi-turn vulnerability and show that the most capable model is the most sycophantic, consistent with alignment-induced user-deference bias.
    \item We design a diagnostic intervention experiment that disambiguates alignment attenuation from context information loss, finding that alignment reminders reduce sycophancy by 68\% while context summaries increase it by 63\%.
\end{itemize}
