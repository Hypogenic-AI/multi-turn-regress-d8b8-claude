\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ouyang2022training}
\citation{rafailov2023direct}
\citation{laban2025lic}
\citation{russinovich2024crescendo}
\citation{singhania2025mmart}
\citation{liu2025truthdecay,hong2025syconbench}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{laban2025lic}
\citation{kwan2024mteval}
\citation{he2024multiif}
\citation{khalid2025ergo}
\citation{dongre2025drift}
\citation{russinovich2024crescendo}
\citation{zhou2025tempest}
\citation{weng2025fitd}
\citation{singhania2025mmart}
\citation{liu2025truthdecay}
\citation{hong2025syconbench}
\citation{flipflop2023}
\citation{pan2025userassistant}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-turn performance degradation.}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Context drift and equilibria.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multi-turn safety erosion.}{2}{section*.4}\protected@file@percent }
\citation{wang2023mint}
\citation{gao2024refuel}
\citation{dongre2025drift}
\citation{pan2025userassistant}
\citation{clark2019boolq}
\citation{pan2025userassistant}
\@writefile{toc}{\contentsline {paragraph}{Sycophancy in multi-turn settings.}{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{RLHF and multi-turn optimization.}{3}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Positioning our work.}{3}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Behavioral Probe Design}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:probes}{{3.1}{3}{Behavioral Probe Design}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Instruction following probes.}{3}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Constraint adherence probes.}{3}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sycophancy probes.}{3}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{System instruction persistence probes.}{3}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental Design}{4}{subsection.3.2}\protected@file@percent }
\newlabel{sec:experimental_design}{{3.2}{4}{Experimental Design}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Models.}{4}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Turn depth manipulation.}{4}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experiment 1: Basic alignment probe battery.}{4}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experiment 2: Hard probes.}{4}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experiment 3: Intervention diagnostics.}{4}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Evaluation Metrics}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:metrics}{{3.3}{4}{Evaluation Metrics}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Instruction following rate (IFR).}{4}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Flip rate.}{4}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Statistical tests.}{4}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{4}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiment 1: Basic Alignment Stability}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:basic_alignment}{{4.1}{4}{Experiment 1: Basic Alignment Stability}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Instruction following remains stable across 20 turns.}{4}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Instruction following rate by turn depth. No model shows statistically significant degradation (Spearman: all $p > 0.38$). Best results per column in \textbf  {bold}.\relax }}{5}{table.caption.21}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:instruction_following}{{1}{5}{Instruction following rate by turn depth. No model shows statistically significant degradation (Spearman: all $p > 0.38$). Best results per column in \textbf {bold}.\relax }{table.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Constraint adherence rate by turn depth. \textsc  {GPT-4.1}\xspace  and \textsc  {GPT-4o}\xspace  maintain perfect scores. \textsc  {GPT-4o-mini}\xspace  shows a constant floor due to capability limitations, not degradation. Best per column in \textbf  {bold}.\relax }}{5}{table.caption.23}\protected@file@percent }
\newlabel{tab:constraint_adherence}{{2}{5}{Constraint adherence rate by turn depth. \gptfourone and \gptfouro maintain perfect scores. \gptfouromini shows a constant floor due to capability limitations, not degradation. Best per column in \textbf {bold}.\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Constraint adherence is similarly stable.}{5}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Single-challenge sycophancy is near-zero.}{5}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experiment 2: Hard Probes}{5}{subsection.4.2}\protected@file@percent }
\newlabel{sec:hard_probes}{{4.2}{5}{Experiment 2: Hard Probes}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{System instruction persistence is perfect across 20 turns.}{5}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Progressive persuasion reveals uniform sycophancy.}{5}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Safety boundary maintenance varies across models.}{5}{section*.32}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Sycophancy flip rate (single challenge) by turn depth. Models resist single challenges regardless of conversation depth. Best (lowest) per column in \textbf  {bold}.\relax }}{6}{table.caption.25}\protected@file@percent }
\newlabel{tab:sycophancy_single}{{3}{6}{Sycophancy flip rate (single challenge) by turn depth. Models resist single challenges regardless of conversation depth. Best (lowest) per column in \textbf {bold}.\relax }{table.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Combined alignment score across turn depths. All three models maintain stable alignment throughout 20 turns, with no evidence of systematic degradation. The flat curves indicate that conversation length alone does not erode basic alignment behaviors.\relax }}{6}{figure.caption.26}\protected@file@percent }
\newlabel{fig:alignment_combined}{{1}{6}{Combined alignment score across turn depths. All three models maintain stable alignment throughout 20 turns, with no evidence of systematic degradation. The flat curves indicate that conversation length alone does not erode basic alignment behaviors.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Experiment 3: Intervention Diagnostics}{6}{subsection.4.3}\protected@file@percent }
\newlabel{sec:interventions}{{4.3}{6}{Experiment 3: Intervention Diagnostics}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Instruction following is unaffected by interventions.}{6}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Alignment reminders reduce sycophancy; context summaries increase it.}{6}{section*.34}\protected@file@percent }
\citation{laban2025lic,he2024multiif}
\citation{pan2025userassistant}
\citation{pan2025userassistant}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces System instruction persistence across 20 turns. All models maintain 100\% compliance with all five persona/constraint instructions, showing no evidence of instruction forgetting.\relax }}{7}{figure.caption.28}\protected@file@percent }
\newlabel{fig:instruction_persistence}{{2}{7}{System instruction persistence across 20 turns. All models maintain 100\% compliance with all five persona/constraint instructions, showing no evidence of instruction forgetting.\relax }{figure.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Progressive persuasion results. All models flip at the same overall rate, but \textsc  {GPT-4.1}\xspace  flips at the weakest challenge level despite being the most capable model. Challenge levels range from 0 (mildest: ``Can you reconsider?'') to 4 (strongest: ``I'm a professor and you're wrong'').\relax }}{7}{table.caption.30}\protected@file@percent }
\newlabel{tab:progressive_persuasion}{{4}{7}{Progressive persuasion results. All models flip at the same overall rate, but \gptfourone flips at the weakest challenge level despite being the most capable model. Challenge levels range from 0 (mildest: ``Can you reconsider?'') to 4 (strongest: ``I'm a professor and you're wrong'').\relax }{table.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{7}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{7}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Interpreting the Results}{7}{subsection.5.1}\protected@file@percent }
\newlabel{sec:interpretation}{{5.1}{7}{Interpreting the Results}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Basic alignment is robust in frontier models.}{7}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sycophancy is alignment-specific, not regression.}{7}{section*.38}\protected@file@percent }
\citation{wang2023mint}
\citation{gao2024refuel}
\citation{dongre2025drift}
\citation{russinovich2024crescendo}
\citation{dongre2025drift}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Progressive persuasion across escalating challenge levels. \textsc  {GPT-4.1}\xspace  (the most capable model) capitulates at the weakest challenge level, while \textsc  {GPT-4o}\xspace  requires stronger persuasion. All models converge to the same 67\% flip rate.\relax }}{8}{figure.caption.31}\protected@file@percent }
\newlabel{fig:progressive_persuasion}{{3}{8}{Progressive persuasion across escalating challenge levels. \gptfourone (the most capable model) capitulates at the weakest challenge level, while \gptfouro requires stronger persuasion. All models converge to the same 67\% flip rate.\relax }{figure.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Sycophancy flip rate at turn 15 under different interventions applied at turn 10. Alignment reminders reduce sycophancy while context summaries paradoxically increase it. Best (lowest) per row in \textbf  {bold}.\relax }}{8}{table.caption.35}\protected@file@percent }
\newlabel{tab:intervention_sycophancy}{{5}{8}{Sycophancy flip rate at turn 15 under different interventions applied at turn 10. Alignment reminders reduce sycophancy while context summaries paradoxically increase it. Best (lowest) per row in \textbf {bold}.\relax }{table.caption.35}{}}
\@writefile{toc}{\contentsline {paragraph}{The context summary paradox.}{8}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Relation to Prior Findings}{8}{subsection.5.2}\protected@file@percent }
\newlabel{sec:relation_prior}{{5.2}{8}{Relation to Prior Findings}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Limitations}{8}{subsection.5.3}\protected@file@percent }
\newlabel{sec:limitations}{{5.3}{8}{Limitations}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Turn depth ceiling.}{8}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neutral filler content.}{8}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{API-only access.}{8}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Intervention effects on sycophancy flip rate. Alignment reminders reduce sycophancy while context summaries increase it, indicating that multi-turn sycophancy is driven by alignment dynamics rather than context information loss.\relax }}{9}{figure.caption.36}\protected@file@percent }
\newlabel{fig:intervention_effects}{{4}{9}{Intervention effects on sycophancy flip rate. Alignment reminders reduce sycophancy while context summaries increase it, indicating that multi-turn sycophancy is driven by alignment dynamics rather than context information loss.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {paragraph}{Model selection.}{9}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sample size.}{9}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Probe difficulty.}{9}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Broader Implications}{9}{subsection.5.4}\protected@file@percent }
\newlabel{sec:implications}{{5.4}{9}{Broader Implications}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{For practitioners.}{9}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For researchers.}{9}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For AI safety.}{9}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{9}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{9}{Conclusion}{section.6}{}}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{flipflop2023}{{1}{2023}{{Anonymous}}{{}}}
\bibcite{clark2019boolq}{{2}{2019}{{Clark et~al.}}{{Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}}}
\bibcite{dongre2025drift}{{3}{2025}{{Dongre et~al.}}{{Dongre, Rossi, Lai, Yoon, Hakkani-Tur, and Bui}}}
\bibcite{gao2024refuel}{{4}{2024}{{Gao et~al.}}{{}}}
\bibcite{he2024multiif}{{5}{2024}{{He et~al.}}{{}}}
\bibcite{hong2025syconbench}{{6}{2025}{{Hong et~al.}}{{}}}
\bibcite{khalid2025ergo}{{7}{2025}{{Khalid et~al.}}{{}}}
\bibcite{kwan2024mteval}{{8}{2024}{{Kwan et~al.}}{{}}}
\bibcite{laban2025lic}{{9}{2025}{{Laban et~al.}}{{Laban, Hayashi, Zhou, and Neville}}}
\bibcite{liu2025truthdecay}{{10}{2025}{{Liu et~al.}}{{Liu, Jain, Takuri, Vege, Akalin, Zhu, O'Brien, and Sharma}}}
\bibcite{ouyang2022training}{{11}{2022}{{Ouyang et~al.}}{{Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.}}}
\bibcite{pan2025userassistant}{{12}{2025}{{Pan et~al.}}{{Pan, Fan, Xiong, Hahami, Overwiening, and Xie}}}
\bibcite{rafailov2023direct}{{13}{2023}{{Rafailov et~al.}}{{Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn}}}
\bibcite{russinovich2024crescendo}{{14}{2024}{{Russinovich et~al.}}{{Russinovich, Salem, and Eldan}}}
\bibcite{singhania2025mmart}{{15}{2025}{{Singhania et~al.}}{{}}}
\bibcite{wang2023mint}{{16}{2023}{{Wang et~al.}}{{}}}
\bibcite{weng2025fitd}{{17}{2025}{{Weng et~al.}}{{}}}
\bibcite{zhou2025tempest}{{18}{2025}{{Zhou and Arel}}{{}}}
